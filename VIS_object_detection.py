#!/usr/bin/env python
# coding: utf-8
"""
Detect Objects Using Your Webcam
================================
"""

import tarfile
import urllib.request
import os
import sys
PATH_TO_CKPT = "E:\\TensorFlow\\workspace\\training_demo\\exported-models-resnet\\checkpoint\\"
PATH_TO_CFG = "E:\\TensorFlow\\workspace\\training_demo\\exported-models-resnet\\pipeline.config"


# Download labels file
PATH_TO_LABELS = "E:\\TensorFlow\\workspace\\training_demo\\annotations\\label_map2.pbtxt"

# %%
# Load the model
# ~~~~~~~~~~~~~~
# Next we load the downloaded model

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging
import tensorflow as tf
from object_detection.utils import label_map_util
from object_detection.utils import config_util
from object_detection.utils import visualization_utils as viz_utils
from object_detection.builders import model_builder

tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)

# Enable GPU dynamic memory allocation
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)
model_config = configs['model']
detection_model = model_builder.build(model_config=model_config, is_training=False)

# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()

@tf.function
def detect_fn(image):
    """Detect objects in image."""

    image, shapes = detection_model.preprocess(image)
    prediction_dict = detection_model.predict(image, shapes)
    detections = detection_model.postprocess(prediction_dict, shapes)

    return detections, prediction_dict, tf.reshape(shapes, [-1])


# %%
# Load label map data (for plotting)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Label maps correspond index numbers to category names, so that when our convolution network
# predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility
# functions, but anything that returns a dictionary mapping integers to appropriate string labels
# would be fine.
category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,
                                                                    use_display_name=True)

# %%
# Define the video stream
# ~~~~~~~~~~~~~~~~~~~~~~~
# We will use `OpenCV <https://pypi.org/project/opencv-python/>`_ to capture the video stream
# generated by our webcam. For more information you can refer to the `OpenCV-Python Tutorials <https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html#capture-video-from-camera>`_
import cv2

# cap = cv2.VideoCapture(0)

import numpy as np
import pandas as pd
def manual_label_video(video_file, outLabel):
    """
    This function processes a video, detects objects in each frame, and logs the 
    frame number, x and y coordinates of the center of the detection box with 
    the highest detection score greater than 0.7.
    """
    cap = cv2.VideoCapture(video_file)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_number_count = 0
    print (fps)
    log_list = []
    cv2.namedWindow('image')
    
    while cap.isOpened():
        # Read video capture
        ret, frame = cap.read()
        if not ret:
            break

    
        input_tensor = tf.convert_to_tensor(np.expand_dims(frame, 0), dtype=tf.float32)

        # Detect objects in the frame
        detections, predictions_dict, shapes = detect_fn(input_tensor)

        # Get detection boxes, scores, and classes
        detection_boxes = detections['detection_boxes'][0].numpy()
        detection_scores = detections['detection_scores'][0].numpy()
        print (detection_boxes)
        print (detection_scores)
        print (detections['detection_classes'][0].numpy())
        label_id_offset=1
        # Filter boxes with detection scores > 0.7
        valid_detections = [(box, score) for box, score in zip(detection_boxes, detection_scores) if score > 0]
        viz_utils.visualize_boxes_and_labels_on_image_array(
              frame,
              detections['detection_boxes'][0].numpy(),
              (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),
              detections['detection_scores'][0].numpy(),
              category_index,
              use_normalized_coordinates=True,
              max_boxes_to_draw=20,
              min_score_thresh=.2,
              agnostic_mode=False)
        key = cv2.waitKey(0) & 0xFF
        if key == ord('q'):
            cap.release()
            cv2.destroyAllWindows()
            return
        # Press any key to proceed to the next frame, 'q' to quit
        elif key == ord(' '):

            pass       
        elif key == ord('+'):
            
            for i in range(100):
                cap.read()
            frame_number_count+=100
            pass
        else:
            pass
        frame_number_count += 1    
        # if valid_detections:
        #     # Find the box with the maximum score
        #     max_box, max_score = max(valid_detections, key=lambda x: x[1])

        #     # Compute the center of the detection box
        #     ymin, xmin, ymax, xmax = max_box
        #     image_height, image_width, _ = frame.shape
        #     center_x = int((xmin + xmax) / 2 * image_width)
        #     center_y = int((ymin + ymax) / 2 * image_height)

        #     # Log the frame number and the center coordinates
        #     log_list.append([frame_number_count, center_x, center_y])
        #     print (frame_number_count, center_x, center_y)
        #     # Draw the detection box on the frame
        #     frame = viz_utils.visualize_boxes_and_labels_on_image_array(
        #         frame,
        #         np.expand_dims(max_box, axis=0),
        #         np.array([1]),  # dummy class index
        #         np.expand_dims(max_score, axis=0),
        #         category_index,
        #         use_normalized_coordinates=True,
        #         max_boxes_to_draw=1,
        #         min_score_thresh=0.7,
        #         agnostic_mode=False,
        #         groundtruth_box_visualization_color="red"
        #     )

        # Display the frame with the detected box
        cv2.imshow("image", frame)


        # if key == ord('\n'):
        #     continue
    cap.release()
    cv2.destroyAllWindows()  # destroy all opened windows
    
    # Save the log to a CSV file
    df = pd.DataFrame(log_list, columns=['fname', 'Center X', 'Center Y'])
    df.to_csv(f"{outLabel}.csv", index=False)

    return 1

manual_label_video(sys.argv[1],sys.argv[2])